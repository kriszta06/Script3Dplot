import os
import sys
from pathlib import Path
import json
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import re
from matplotlib.patches import FancyArrowPatch
from mpl_toolkits.mplot3d import proj3d
from sklearn.cluster import AgglomerativeClustering, DBSCAN
from sklearn.preprocessing import StandardScaler
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from scipy.spatial.distance import cdist
from scipy.stats import entropy
from disability_config import DISABILITY_MESSAGES

#sageti pentru a arata directia in care se uita subiectul
#se creeaza o sageata 3D cu coordonatele de start si end
#se converteste in 2D
class Arrow3D(FancyArrowPatch):
    def __init__(self, xs, ys, zs, *args, **kwargs):
        FancyArrowPatch.__init__(self, (xs[0], ys[0]), (xs[1], ys[1]), *args, **kwargs)
        self._verts3d = xs, ys, zs

    def do_3d_projection(self, renderer=None):
        xs3d, ys3d, zs3d = self._verts3d
        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, self.axes.M)
        self.set_positions((xs[0], ys[0]), (xs[1], ys[1]))
        return np.min(zs)

#convertire date de tip string din fisierele json in vectori numpy
def parse_vector_string(vector_str):
    numbers = re.findall(r'-?\d+\.?\d*', vector_str)
    if len(numbers) >= 3:
        return np.array([float(numbers[0]), float(numbers[1]), float(numbers[2])])
    return np.array([0.0, 0.0, 0.0])

def load_complete_head_data(file_path):
    #citire fisiere
    with open(file_path, 'r') as f:
        data = json.load(f)
    #declarari
    recordings = data.get('Recordings', [])
    positions = []
    rotations = []
    forward_vectors = []
    timestamps = []
    for recording in recordings:
        #extragere pozitii cap
        head_pos_str = recording.get('HeadPosition', '(0.00, 0.00, 0.00)')
        position = parse_vector_string(head_pos_str)
        positions.append(position)
        #extragere rotatii cap
        head_rot_str = recording.get('HeadRotation', '(0.00, 0.00, 0.00)')
        rotation = parse_vector_string(head_rot_str)
        rotations.append(rotation)
        #extragere vectori de directie cap
        head_forward_str = recording.get('HeadForward', '(0.00, 0.00, 1.00)')
        forward = parse_vector_string(head_forward_str)
        forward_vectors.append(forward)
        #extragere timestamp
        scene_time = recording.get('SceneTime', 0.0)
        timestamps.append(scene_time)
    #convertire liste in numpy arrays
    positions = np.array(positions)
    rotations = np.array(rotations)
    forward_vectors = np.array(forward_vectors)
    timestamps = np.array(timestamps)
    if len(timestamps) == 0:
        return np.array([]), np.array([]), np.array([]), np.array([])
    total_duration = timestamps[-1] - timestamps[0] if len(timestamps) > 0 else 0
    if total_duration <= 20:
        return np.array([]), np.array([]), np.array([]), np.array([])
    #ignorare primele si ultimele 10 secunde
    start_time = timestamps[0] + 10.0
    end_time = timestamps[-1] - 10.0
    valid_indices = (timestamps >= start_time) & (timestamps <= end_time)
    return positions[valid_indices], rotations[valid_indices], forward_vectors[valid_indices], timestamps[valid_indices]

#clustering hierarhic pentru analiza detaliata a capului si dendograme
def perform_agglomerative_clustering(data, scenario_name, features_combination):
    if len(data) == 0:
        print(f"No valid data for {scenario_name} - {features_combination}")
        return None
    
    print(f"\n=== CLUSTERING ANALYSIS for {scenario_name} ===")
    print(f"Features analyzed: {features_combination}")
    print(f"Total number of points: {len(data)}")
    
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data)
    
    # Dendrograme
    Z = linkage(scaled_data, 'ward')
    plt.figure(figsize=(15, 8))
    dendrogram(Z, truncate_mode='level', p=5)
    plt.title(f'Dendrogram for {scenario_name}\n(Shows how similar points are grouped)', fontsize=14, fontweight='bold')
    plt.xlabel('Point index', fontsize=12)
    plt.ylabel('Distance between groups', fontsize=12)
    
    # Explicatii dendograme
    plt.figtext(0.02, 0.02, 
                "DENDROGRAM EXPLANATION:\n"
                "‚Ä¢ Each horizontal line = a group of similar points\n"
                "‚Ä¢ Lower lines = more similar points\n"
                "‚Ä¢ Vertical lines show how groups combine\n"
                "‚Ä¢ Large groups (long lines) = big differences between areas",
                fontsize=10, bbox=dict(facecolor='lightblue', alpha=0.8))
    
    plt.savefig(f'dendrogram_{scenario_name}_{features_combination.replace("+", "_")}.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # Clustere
    n_clusters = 3
    clustering = AgglomerativeClustering(
        n_clusters=n_clusters,
        metric='euclidean',
        linkage='ward'
    )
    cluster_labels = clustering.fit_predict(scaled_data)
    
    # Analiza clustere
    print(f"\nCLUSTERING RESULTS:")
    print(f"Number of groups created: {n_clusters}")
    
    for i in range(n_clusters):
        cluster_size = np.sum(cluster_labels == i)
        percentage = (cluster_size / len(cluster_labels)) * 100
        print(f"  Group {i}: {cluster_size} points ({percentage:.1f}%)")
    
    if 'Position' in features_combination:
        print(f"\nCLUSTER INTERPRETATION (head positions):")
        positions = data[:, :3] if data.shape[1] >= 3 else data
        for i in range(n_clusters):
            cluster_positions = positions[cluster_labels == i]
            if len(cluster_positions) > 0:
                center = np.mean(cluster_positions, axis=0)
                print(f"  Group {i}: Center at position {center}")
                if len(cluster_positions) > 1:
                    spread = np.std(cluster_positions, axis=0)
                    print(f"           Spread: {spread}")
    
    return cluster_labels

#creeaza un plot 3D pentru clusterele detectate
def plot_clusters(positions, labels, scenario_name, combo_name):
    fig = plt.figure(figsize=(16, 10))
    ax = fig.add_subplot(111, projection='3d')
    
    unique_labels = np.unique(labels)
    colors = ['red', 'blue', 'green', 'orange', 'purple'][:len(unique_labels)]
    
    cluster_stats = []
    
    for label, color in zip(unique_labels, colors):
        idx = labels == label
        cluster_positions = positions[idx]
        
        center = np.mean(cluster_positions, axis=0)
        size = len(cluster_positions)
        percentage = (size / len(positions)) * 100
        
        cluster_stats.append({
            'label': label,
            'color': color,
            'center': center,
            'size': size,
            'percentage': percentage
        })
        
        ax.scatter(
            cluster_positions[:, 0], cluster_positions[:, 2], cluster_positions[:, 1],
            c=[color], label=f'Area {label} ({size} points, {percentage:.1f}%)', 
            s=50, alpha=0.7
        )
        
        #Marcaj central pt clustere
        ax.scatter(center[0], center[2], center[1], 
                   c='black', s=200, marker='x', linewidth=3, zorder=10)
    
    ax.set_xlabel('X Position', fontsize=12, fontweight='bold')
    ax.set_ylabel('Z Position', fontsize=12, fontweight='bold')
    ax.set_zlabel('Y Position', fontsize=12, fontweight='bold')
    ax.set_title(f'Head Position Grouping - {scenario_name}\n(Areas where user spent most time)', 
                 fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.5)
    ax.view_init(elev=30, azim=120)
    
    explanation = (
        "VISUALIZATION EXPLANATION:\n"
        "‚Ä¢ Each color = an area where user was active\n"
        "‚Ä¢ Large black points (X) = center of each area\n"
        "‚Ä¢ More points in an area = more time spent there\n"
        "‚Ä¢ Areas with many points = main areas of interest\n"
        "‚Ä¢ Areas with few points = passing through or uninteresting areas"
    )
    
    ax.text2D(0.02, 0.98, explanation,
              transform=ax.transAxes, fontsize=10,
              va='top', bbox=dict(facecolor='lightyellow', alpha=0.9, edgecolor='gray'))
    
    plt.savefig(f'clusters_{scenario_name}_{combo_name.replace("+", "_")}.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    #Afisare statistici pentru fiecare zona de interes
    print(f"\nINTEREST AREA STATISTICS for {scenario_name}:")
    for stat in cluster_stats:
        print(f"  Area {stat['label']} ({stat['color']}):")
        print(f"    - {stat['size']} points ({stat['percentage']:.1f}% of total)")
        print(f"    - Center: X={stat['center'][0]:.2f}, Y={stat['center'][1]:.2f}, Z={stat['center'][2]:.2f}")
        if stat['percentage'] > 30:
            print(f"    - ‚≠ê MAIN AREA (user spent a lot of time here)")
        elif stat['percentage'] < 10:
            print(f"    - ‚ö†Ô∏è MINOR AREA (user spent little time here)")
        else:
            print(f"    - üìç MODERATE AREA (medium activity)")
        print()
    
    return cluster_stats

def create_detailed_head_analysis(file_path, save_plot=True, save_csv=False, show_plot=False, global_origin=None):
    positions, rotations, forward_vectors, timestamps = load_complete_head_data(file_path)
    if len(positions) == 0:
        print(f"No valid data found in {file_path}")
        return
    
    print(f"\n{'='*60}")
    print(f"DETAILED ANALYSIS: {Path(file_path).stem}")
    print(f"{'='*60}")
    
    #Normalizare a pozi»õiilor
    if global_origin is not None and positions.shape[1] == global_origin.shape[0]:
        positions = positions - global_origin
    else:
        origin = positions[0]
        positions = positions - origin

    N = 10
    positions = positions[::N]
    rotations = rotations[::N]
    forward_vectors = forward_vectors[::N]
    timestamps = timestamps[::N]

    print(f"üìä GENERAL STATISTICS:")
    print(f"   ‚Ä¢ Total duration: {timestamps[-1] - timestamps[0]:.1f} seconds")
    print(f"   ‚Ä¢ Number of points analyzed: {len(positions)}")
    print(f"   ‚Ä¢ Total distance traveled: {np.sum(np.linalg.norm(np.diff(positions, axis=0), axis=1)):.2f} units")
    
    #Analiza pozitiilor
    movements = np.diff(positions, axis=0)
    movement_speeds = np.linalg.norm(movements, axis=1)
    print(f"   ‚Ä¢ Average movement speed: {np.mean(movement_speeds):.3f} units/step")
    print(f"   ‚Ä¢ Maximum speed: {np.max(movement_speeds):.3f} units/step")
    
    #Detectare viraje bru»ôte
    turn_indices = []
    for i in range(2, len(positions)):
        v1 = positions[i-1] - positions[i-2]
        v2 = positions[i] - positions[i-1]
        if np.linalg.norm(v1) > 1e-6 and np.linalg.norm(v2) > 1e-6:
            angle = np.arccos(np.clip(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)), -1.0, 1.0))
            if np.degrees(angle) > 30:
                turn_indices.append(i-1)
    print(f"   ‚Ä¢ Number of sharp turns (>30¬∞): {len(turn_indices)}")
    
    scenario_name = Path(file_path).stem
    feature_combinations = {
        "HeadPosition": positions,
        "HeadRotation": rotations,
        "HeadForward": forward_vectors,
        "SceneTimes": timestamps.reshape(-1, 1),
        "HeadPosition+HeadRotation": np.hstack((positions, rotations)),
        "HeadPosition+HeadForward": np.hstack((positions, forward_vectors)),
        "HeadRotation+HeadForward": np.hstack((rotations, forward_vectors)),
        "HeadPosition+HeadRotation+HeadForward": np.hstack((positions, rotations, forward_vectors)),
        "HeadPosition+HeadRotation+HeadForward+SceneTimes": np.hstack((positions, rotations, forward_vectors, timestamps.reshape(-1, 1)))
    }
    
    print(f"\nüîç CLUSTERING ANALYSIS:")
    print(f"   Will analyze {len(feature_combinations)} feature combinations...")
    
    cluster_stats_list = []
    for combo_name, combo_data in feature_combinations.items():
        cluster_labels = perform_agglomerative_clustering(combo_data, scenario_name, combo_name)
        if cluster_labels is not None:
            np.savez(
                f'clustering_results_{scenario_name}_{combo_name.replace("+", "_")}.npz',
                labels=cluster_labels,
                features=combo_data
            )
            if 'Position' in combo_name:
                stats = plot_clusters(positions, cluster_labels, scenario_name, combo_name)
                cluster_stats_list.append(stats)
    
    # 3D plot
    fig = plt.figure(figsize=(22, 10))
    ax = fig.add_subplot(111, projection='3d')
    x = positions[:, 0]
    y = positions[:, 2]  
    z = positions[:, 1]  

    ax.plot(x, y, z,
            color='deepskyblue', linewidth=2.5, alpha=0.95, linestyle='-', label='Head route')
    ax.scatter(x[0], y[0], z[0],
               c='lime', s=220, label='START', marker='o', edgecolors='black', linewidth=3, zorder=10)
    ax.scatter(x[-1], y[-1], z[-1],
               c='red', s=220, label='END', marker='*', edgecolors='black', linewidth=3, zorder=10)

    #Sageti
    step = max(1, len(positions) // 25)
    arrow_count = 0
    for i in range(0, len(positions), step):
        pos = positions[i]
        forward = forward_vectors[i]
        arrow_start = [pos[0], pos[2], pos[1]]
        arrow_end = [pos[0] + forward[0]*0.4, pos[2] + forward[2]*0.4, pos[1] + forward[1]*0.4]
        # Transmitere directƒÉ a coordonatelor
        arrow = Arrow3D(
            (arrow_start[0], arrow_end[0]),
            (arrow_start[1], arrow_end[1]),
            (arrow_start[2], arrow_end[2]), 
            mutation_scale=22, lw=2.5, arrowstyle="-|>", color="crimson", alpha=0.7)
        ax.add_artist(arrow)
        arrow_count += 1

    #Marcaje pentru unghiuri (triunghiurile galbene)
    if turn_indices:
        ax.scatter(x[turn_indices], y[turn_indices], z[turn_indices],
                   c='orange', s=80, marker='^', label=f'Turns ({len(turn_indices)} found)', zorder=9)

    ax.set_xlabel('X Position', fontsize=14, fontweight='bold')
    ax.set_ylabel('Z Position', fontsize=14, fontweight='bold')
    ax.set_zlabel('Y Position', fontsize=14, fontweight='bold')
    ax.set_title(f'3D Head Movement Route - {scenario_name}', fontsize=18, fontweight='bold')
    ax.legend(fontsize=12)
    ax.grid(True, alpha=0.5)
    ax.view_init(elev=30, azim=120)
    
    route_explanation = (
        "3D ROUTE EXPLANATION:\n"
        "‚Ä¢ Blue line = complete head movement path\n"
        "‚Ä¢ Red arrows = where user was looking\n"
        "‚Ä¢ Orange triangles = sharp turns (>30¬∞)\n"
        "‚Ä¢ Green point = start of experience\n"
        "‚Ä¢ Red point = end of experience\n"
        f"‚Ä¢ Number of arrows shown: {arrow_count}"
    )
    
    ax.text2D(0.02, 0.98, route_explanation,
              transform=ax.transAxes, fontsize=11,
              va='top', bbox=dict(facecolor='lightblue', alpha=0.9, edgecolor='gray'))
    
    plt.tight_layout()

    if save_plot:
        output_file = f"detailed_analysis_{scenario_name}_3d.png"
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"\nüíæ 3D route saved as: {output_file}")

    if show_plot:
        plt.show()
    else:
        plt.close()
    
    print(f"\n‚úÖ Analysis for {scenario_name} complete!")
    print(f"{'='*60}\n")
    
    #statistici generale
    return {
        'scenario_name': scenario_name,
        'positions': positions,
        'rotations': rotations,
        'forward_vectors': forward_vectors,
        'timestamps': timestamps,
        'total_distance': np.sum(np.linalg.norm(np.diff(positions, axis=0), axis=1)),
        'avg_speed': np.mean(movement_speeds),
        'max_speed': np.max(movement_speeds),
        'num_sharp_turns': len(turn_indices),
        'cluster_stats': cluster_stats_list
    }

#compara»õie √Æntre scenarii 
def plot_all_scenarios_comparison(all_data, global_origin=None):
    if not all_data:
        print("No data available for plotting.")
        return

    fig = plt.figure(figsize=(16, 10))
    ax = fig.add_subplot(111, projection='3d')

    colors = [plt.cm.tab10(i) for i in range(len(all_data))]

    for idx, (scenario_name, data) in enumerate(all_data.items()):
        # Extrage pozitiile din primele 3 coloane
        positions = data[:, :3]

        # Normalizare
        if global_origin is not None:
            positions = positions - global_origin
        else:
            positions = positions - positions[0]

        x = positions[:, 0]
        y = positions[:, 2] 
        z = positions[:, 1] 

        ax.plot(x, y, z,
                label=scenario_name,
                linewidth=2.0,
                alpha=0.85,
                color=colors[idx])

        #start
        ax.scatter(x[0], y[0], z[0], marker='o', color=colors[idx], s=100, edgecolors='black')

    ax.set_xlabel("X Position", fontsize=12)
    ax.set_ylabel("Z Position", fontsize=12)
    ax.set_zlabel("Y Position", fontsize=12)
    ax.set_title("Comparison of all scenarios 3D (head routes)", fontsize=16, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.4)
    ax.view_init(elev=30, azim=120)
    plt.tight_layout()

#extragere caracteristici din datele de cap pentru detectarea dizabilitƒÉ»õilor
def extract_behavior_features(scenario_data):
    positions = scenario_data[:, :3]
    rotations = scenario_data[:, 3:6]
    forwards = scenario_data[:, 6:9]
    timestamps = scenario_data[:, -1]
    
    features = []
    
    # 1. STATISTICI SPA»öIALE DE BAZƒÇ
    features.extend(np.mean(positions, axis=0))  # Pozi»õia medie
    features.extend(np.std(positions, axis=0))   # Varia»õia pozi»õiei
    features.append(np.sum(np.linalg.norm(np.diff(positions, axis=0), axis=0)))  # Distan»õa totalƒÉ
    
    # 2. STATISTICI DE ORIENTARE
    features.extend(np.mean(rotations, axis=0))  # Rota»õia medie
    features.extend(np.std(rotations, axis=0))   # Varia»õia rota»õiei
    features.extend(np.mean(forwards, axis=0))   # Direc»õia medie
    features.extend(np.std(forwards, axis=0))    # Varia»õia direc»õiei
    
    # 3. STATISTICI TEMPORALE
    total_time = np.max(timestamps) - np.min(timestamps)
    features.append(total_time)  # Durata totalƒÉ
    features.append(np.mean(np.diff(timestamps)))  # Timpul mediu √Æntre frame-uri
    
    # 4. ANALIZA MI»òCƒÇRII - IMPORTANTƒÇ PENTRU DIZABILITƒÇ»öI
    movements = np.diff(positions, axis=0)
    movement_speeds = np.linalg.norm(movements, axis=1)
    
    # Viteza medie »ôi varia»õia vitezei
    features.append(np.mean(movement_speeds))
    features.append(np.std(movement_speeds))
    features.append(np.max(movement_speeds))
    
    # Accelerarea (schimbarea vitezei)
    if len(movement_speeds) > 1:
        acceleration = np.diff(movement_speeds)
        features.append(np.mean(np.abs(acceleration)))  # Accelerarea medie
        features.append(np.std(acceleration))           # Varia»õia accelera»õiei
    else:
        features.extend([0, 0])
    
    # 5. ENTROPIA MI»òCƒÇRILOR - MƒÇSURƒÇ A COMPLEXITƒÇ»öII
    if len(movements) > 0:
        hist, _ = np.histogramdd(movements, bins=5)
        hist = hist.flatten()
        total = np.sum(hist)
        if total > 0:
            hist = hist / total
            movement_entropy = -np.sum(hist * np.log(hist + 1e-10))
        else:
            movement_entropy = 0
    else:
        movement_entropy = 0
    features.append(movement_entropy)
    
    # 6. RAPORT DISTAN»öƒÇ/TIMP - EFICIEN»öA MI»òCƒÇRII
    total_distance = np.sum(movement_speeds)
    features.append(total_distance / total_time if total_time > 0 else 0)
    
    # 7. ANALIZA UNGHIURILOR - IMPORTANTƒÇ PENTRU COORDONARE
    pitch = rotations[:, 0]
    yaw = rotations[:, 1]
    roll = rotations[:, 2]
    
    # Varia»õia unghiurilor
    features.append(np.ptp(pitch))  # Range-ul pitch
    features.append(np.ptp(yaw))    # Range-ul yaw
    features.append(np.ptp(roll))   # Range-ul roll
    
    # Stabilitatea unghiurilor (varia»õia micƒÉ = mai stabil)
    features.append(np.std(pitch))
    features.append(np.std(yaw))
    features.append(np.std(roll))
    
    # 8. DETECTAREA VIRAJELOR BRU»òTE - INDICATOR DE DIZABILITATE
    sharp_turns = 0
    if len(positions) > 2:
        for i in range(2, len(positions)):
            v1 = positions[i-1] - positions[i-2]
            v2 = positions[i] - positions[i-1]
            if np.linalg.norm(v1) > 1e-6 and np.linalg.norm(v2) > 1e-6:
                angle = np.arccos(np.clip(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)), -1.0, 1.0))
                if np.degrees(angle) > 30:  # Viraj > 30 grade
                    sharp_turns += 1
    features.append(sharp_turns)
    
    # 9. CONSISTEN»öA MI»òCƒÇRII - PATTERN-URI REGULATE VS. IRREGULATE
    if len(movement_speeds) > 1:
        # Coeficientul de varia»õie (CV) - mƒÉsoarƒÉ consisten»õa
        cv = np.std(movement_speeds) / np.mean(movement_speeds) if np.mean(movement_speeds) > 0 else 0
        features.append(cv)
        
        # Autocorelarea - mƒÉsoarƒÉ pattern-urile repetitive
        if len(movement_speeds) > 10:
            autocorr = np.corrcoef(movement_speeds[:-1], movement_speeds[1:])[0, 1]
            features.append(autocorr if not np.isnan(autocorr) else 0)
        else:
            features.append(0)
    else:
        features.extend([0, 0])
    
    # 10. EFFICIENCY METRICS - C√ÇT DE EFICIENT ESTE COMPORTAMENTUL
    # Raportul √Æntre distan»õa realƒÉ »ôi distan»õa euclidianƒÉ
    if len(positions) > 1:
        direct_distance = np.linalg.norm(positions[-1] - positions[0])
        actual_distance = total_distance
        efficiency = direct_distance / actual_distance if actual_distance > 0 else 0
        features.append(efficiency)
    else:
        features.append(0)
    
    # 11. STABILITATEA POZI»öIEI - C√ÇT DE MULT SE MI»òCƒÇ CAPUL
    # Varia»õia pozi»õiei √Æn timp
    position_variance = np.var(positions, axis=0)
    features.extend(position_variance)
    
    # 12. COMPLEXITATEA TRAIECTORIEI
    # NumƒÉrul de direc»õii diferite
    if len(movements) > 0:
        directions = movements / (np.linalg.norm(movements, axis=1, keepdims=True) + 1e-10)
        unique_directions = np.unique(np.round(directions, 2), axis=0)
        features.append(len(unique_directions))
    else:
        features.append(0)
    
    return np.array(features)

def detect_disability_patterns_unsupervised(all_scenarios):
    # Extrage caracteristici pentru fiecare scenariu
    feature_vectors = []
    scenario_names = []
    
    for name, data in all_scenarios.items():
        features = extract_behavior_features(data)
        feature_vectors.append(features)
        scenario_names.append(name)
    
    feature_matrix = np.array(feature_vectors)
    
    # NormalizeazƒÉ caracteristicile
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(feature_matrix)
    
    # Reducere dimensionalitate cu PCA
    pca = PCA(n_components=0.95)  # PƒÉstreazƒÉ 95% din varian»õƒÉ
    principal_components = pca.fit_transform(scaled_features)
    
    # Clusterizare DBSCAN
    clustering = DBSCAN(eps=1.5, min_samples=2).fit(principal_components)
    labels = clustering.labels_
    
    # CalculeazƒÉ distan»õe fa»õƒÉ de centroidul principal
    centroid = np.mean(principal_components, axis=0)
    distances = cdist(principal_components, [centroid]).flatten()
    
    # VizualizeazƒÉ spa»õiul PCA
    plt.figure(figsize=(12, 8))
    unique_labels = np.unique(labels)
    
    for label in unique_labels:
        if label == -1:
            mask = labels == label
            plt.scatter(principal_components[mask, 0], principal_components[mask, 1], 
                        c='gray', s=100, alpha=0.6, label='Outliers')
        else:
            mask = labels == label
            plt.scatter(principal_components[mask, 0], principal_components[mask, 1], 
                        s=150, alpha=0.7, label=f'Group {label}')
    
    # AdaugƒÉ numele scenariilor »ôi distan»õele
    for i, name in enumerate(scenario_names):
        plt.annotate(f"{name}\n({distances[i]:.2f})", 
                    (principal_components[i, 0], principal_components[i, 1]),
                    textcoords="offset points", 
                    xytext=(0,10), 
                    ha='center',
                    fontsize=9)
    
    plt.title('Behavioral Pattern Space (PCA)', fontsize=16)
    plt.xlabel('Principal Component 1', fontsize=12)
    plt.ylabel('Principal Component 2', fontsize=12)
    plt.legend()
    plt.grid(alpha=0.2)
    plt.savefig('disability_pattern_space.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # CalculeazƒÉ scor de dizabilitate
    disability_likelihood = {}
    max_distance = np.max(distances)
    
    for i, name in enumerate(scenario_names):
        disability_score = float(distances[i] / max_distance)  # Convertim la float Python
        status = 'HIGH' if disability_score > 0.7 else 'MEDIUM' if disability_score > 0.4 else 'LOW'
        
        disability_likelihood[name] = {
            'cluster': int(labels[i]),          # Convertim la int Python
            'distance': float(distances[i]),    # Convertim la float Python
            'score': disability_score,
            'status': status
        }
    return disability_likelihood

def plot_disability_annotations(all_data, disability_likelihood, global_origin=None):
    """AdaugƒÉ marker vizual √Æn plot-ul 3D pentru dizabilitƒÉ»õi"""
    fig = plt.figure(figsize=(16, 10))
    ax = fig.add_subplot(111, projection='3d')
    
    # Culori bazate pe scorul de dizabilitate
    colors = {'HIGH': 'red', 'MEDIUM': 'orange', 'LOW': 'green'}
    
    for scenario_name, data in all_data.items():
        # Extrage pozi»õiile (primele 3 coloane)
        positions = data[:, :3]
        
        # NormalizeazƒÉ fa»õƒÉ de global_origin dacƒÉ existƒÉ
        if global_origin is not None:
            positions = positions - global_origin
        else:
            positions = positions - positions[0]
        
        # PloteazƒÉ traseul
        x = positions[:, 0]
        y = positions[:, 2]  
        z = positions[:, 1]  
        
        # Alege culoarea √Æn func»õie de dizabilitate
        status = disability_likelihood[scenario_name]['status']

        ax.plot(x, y, z, color=colors[status], alpha=0.7, linewidth=2.5)
        
        # AdaugƒÉ marker la √Ænceput
        ax.scatter(x[0], y[0], z[0], color=colors[status], s=120, marker='o', edgecolors='black')
        
        # AdaugƒÉ etichetƒÉ cu scorul
        score = disability_likelihood[scenario_name]['score']
        ax.text(x[0], y[0], z[0], 
                f"{scenario_name}: {score:.2f}",
                fontsize=9, zorder=10)
    
    ax.set_xlabel('X Position', fontsize=12, fontweight='bold')
    ax.set_ylabel('Z Position', fontsize=12, fontweight='bold')
    ax.set_zlabel('Y Position', fontsize=12, fontweight='bold')
    ax.set_title('3D Head Paths - Disability Likelihood', fontsize=16, fontweight='bold')
    
    # AdaugƒÉ legendƒÉ
    from matplotlib.lines import Line2D
    legend_elements = [
        Line2D([0], [0], color='red', lw=2, label='High Disability Likelihood'),
        Line2D([0], [0], color='orange', lw=2, label='Medium Disability Likelihood'),
        Line2D([0], [0], color='green', lw=2, label='Low Disability Likelihood')
    ]
    ax.legend(handles=legend_elements, fontsize=10)
    
    # Explica»õie
    explanation = (
        "DISABILITY LIKELIHOOD BASED ON BEHAVIORAL PATTERNS:\n"
        "‚Ä¢ RED: Significantly different behavior (high disability likelihood)\n"
        "‚Ä¢ ORANGE: Moderately different behavior\n"
        "‚Ä¢ GREEN: Typical behavior (low disability likelihood)"
    )
    ax.text2D(0.02, 0.98, explanation, 
              transform=ax.transAxes, fontsize=10,
              bbox=dict(facecolor='white', alpha=0.8))
    
    plt.savefig('disability_3d_assessment.png', dpi=300, bbox_inches='tight')
    plt.close()

def save_detailed_results(disability_assessment, all_person_features):
    """SalveazƒÉ rezultatele detaliate √Æn format JSON"""
    detailed_results = {
        'analysis_timestamp': str(np.datetime64('now')),
        'total_persons': len(disability_assessment),
        'methodology': {
            'pca_components': '95% variance retained',
            'clustering': 'DBSCAN with eps=1.5, min_samples=2',
            'scoring': 'Combined: 50% distance + 30% Mahalanobis + 20% consistency',
            'thresholds': {
                'severe': '>0.7',
                'moderate': '>0.5',
                'light': '>0.3',
                'none': '‚â§0.3'
            }
        },
        'persons': {}
    }
    
    for person_id, result in disability_assessment.items():
        detailed_results['persons'][person_id] = {
            'disability_assessment': result,
            'raw_features': all_person_features[person_id].tolist() if person_id in all_person_features else []
        }
    
    # SalveazƒÉ rezultatele principale
    with open('detailed_disability_assessment.json', 'w', encoding='utf-8') as f:
        json.dump(detailed_results, f, indent=2, ensure_ascii=False)
    
    # SalveazƒÉ un rezumat simplu
    summary = {
        'summary': {
            'total_persons': len(disability_assessment),
            'severe_disability': len([p for p in disability_assessment.values() if p['status'] == 'HIGH']),
            'moderate_disability': len([p for p in disability_assessment.values() if p['status'] == 'MEDIUM']),
            'light_disability': len([p for p in disability_assessment.values() if p['status'] == 'LOW']),
            'no_disability': len([p for p in disability_assessment.values() if p['status'] == 'NONE'])
        },
        'persons': {}
    }
    
    for person_id, result in disability_assessment.items():
        summary['persons'][person_id] = {
            'status': result['status'],
            'disability_level': result['disability_level'],
            'final_score': result['final_score'],
            'risk_factors': result['risk_factors']
        }
    
    with open('disability_summary.json', 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"\nüíæ Rezultatele au fost salvate √Æn:")
    print(f"   ‚Ä¢ detailed_disability_assessment.json - AnalizƒÉ completƒÉ")
    print(f"   ‚Ä¢ disability_summary.json - Rezumat pentru utilizare rapidƒÉ")

def create_visual_disability_report(disability_assessment):
    """CreeazƒÉ un raport vizual simplificat cu doar 2 grafice"""
    try:
        # PregƒÉte»ôte datele pentru grafice
        persons = list(disability_assessment.keys())
        
        # VerificƒÉ dacƒÉ existƒÉ date valide
        if not persons:
            print("‚ùå Nu existƒÉ date pentru generarea raportului vizual")
            return
            
        # Extrage scorurile finale
        final_scores = []
        valid_persons = []
        
        for p in persons:
            try:
                final_score = disability_assessment[p]['final_score']
                # VerificƒÉ dacƒÉ scorurile sunt valide (nu NaN)
                if not np.isnan(final_score):
                    final_scores.append(float(final_score))
                    valid_persons.append(p)
            except (KeyError, ValueError, TypeError) as e:
                print(f"‚ö†Ô∏è Eroare la extragerea datelor pentru {p}: {e}")
                continue
        
        if not valid_persons:
            print("‚ùå Nu existƒÉ date valide pentru generarea graficelelor")
            return
            
        # Status-uri pentru culori
        status_colors = {
            'HIGH': 'red',
            'MEDIUM': 'orange', 
            'LOW': 'yellow',
            'NONE': 'green'
        }
        colors = [status_colors[disability_assessment[p]['status']] for p in valid_persons]
        
        # Figura principalƒÉ cu 2 subplot-uri
        fig = plt.figure(figsize=(16, 8))
        
        # 1. Scorurile finale comparate
        ax1 = plt.subplot(1, 2, 1)
        bars = ax1.bar(range(len(valid_persons)), final_scores, color=colors, alpha=0.7)
        ax1.set_title('Scorurile Finale de Dizabilitate', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Persoane')
        ax1.set_ylabel('Scor Final')
        ax1.set_xticks(range(len(valid_persons)))
        ax1.set_xticklabels([p.replace('Person_', 'P') for p in valid_persons], rotation=45)
        ax1.axhline(y=0.7, color='red', linestyle='--', alpha=0.7, label='Sever (0.7)')
        ax1.axhline(y=0.5, color='orange', linestyle='--', alpha=0.7, label='Moderat (0.5)')
        ax1.axhline(y=0.3, color='yellow', linestyle='--', alpha=0.7, label='U»ôor (0.3)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # AdaugƒÉ valorile pe bare
        for i, (bar, score) in enumerate(zip(bars, final_scores)):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{score:.3f}', ha='center', va='bottom', fontsize=9)
        
        # 2. Rankingul persoanelor √Æn func»õie de severitate
        ax2 = plt.subplot(1, 2, 2)
        sorted_indices = np.argsort(final_scores)[::-1]  # Sortare descrescƒÉtoare
        sorted_scores = [final_scores[i] for i in sorted_indices]
        sorted_persons = [valid_persons[i] for i in sorted_indices]
        sorted_colors = [colors[i] for i in sorted_indices]
        
        bars = ax2.bar(range(len(sorted_persons)), sorted_scores, 
                       color=sorted_colors, alpha=0.7)
        ax2.set_title('Ranking Persoane dupƒÉ Severitatea DizabilitƒÉ»õii', 
                     fontsize=14, fontweight='bold')
        ax2.set_xlabel('Ranking')
        ax2.set_ylabel('Scor Final')
        ax2.set_xticks(range(len(sorted_persons)))
        ax2.set_xticklabels([f"{i+1}. {p.replace('Person_', 'P')}" for i, p in enumerate(sorted_persons)], 
                           rotation=45)
        ax2.axhline(y=0.7, color='red', linestyle='--', alpha=0.7, label='Sever (0.7)')
        ax2.axhline(y=0.5, color='orange', linestyle='--', alpha=0.7, label='Moderat (0.5)')
        ax2.axhline(y=0.3, color='yellow', linestyle='--', alpha=0.7, label='U»ôor (0.3)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # AdaugƒÉ valorile pe bare
        for i, (bar, score) in enumerate(zip(bars, sorted_scores)):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{score:.3f}', ha='center', va='bottom', fontsize=9)
        
        # Titlu general
        plt.suptitle('Raport Vizual DizabilitƒÉ»õi - AnalizƒÉ ComportamentalƒÉ', 
                    fontsize=18, fontweight='bold', y=0.98)
        
        plt.tight_layout()
        plt.savefig('disability_visual_report.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"üìä Raportul vizual simplificat a fost salvat ca: disability_visual_report.png")
        print(f"   ‚Ä¢ {len(valid_persons)} persoane afi»ôate")
        print(f"   ‚Ä¢ Scoruri finale: {min(final_scores):.3f} - {max(final_scores):.3f}")
        
    except Exception as e:
        print(f"‚ùå Eroare la generarea raportului vizual: {e}")
        import traceback
        traceback.print_exc()
       
def group_files_by_person(json_files):
    """GrupeazƒÉ fi»ôierele dupƒÉ identificatorul de persoanƒÉ"""
    grouped = {}
    for file_path in json_files:
        # Extrage identificatorul de persoanƒÉ din formatul '1-A.json', '2-B.json', etc.
        match = re.search(r'(\d+)-[A-Za-z]+\.json$', file_path.name)
        if match:
            person_id = f"Person_{match.group(1)}"
            if person_id not in grouped:
                grouped[person_id] = []
            grouped[person_id].append(file_path)
    
    # Debug: afi»ôeazƒÉ ce s-a gƒÉsit
    print(f"DEBUG: {len(json_files)} fi»ôiere procesate")
    print(f"DEBUG: {len(grouped)} persoane gƒÉsite")
    for person, files in grouped.items():
        print(f"DEBUG: {person}: {len(files)} fi»ôiere")
    
    return grouped

def combine_features_for_person(scenario_features):
    """CombinƒÉ caracteristicile din cele 3 scenarii √Æntr-un singur vector pentru persoanƒÉ"""
    combined = []
    # AdaugƒÉ toate caracteristicile din scenarii √Æntr-un singur vector
    for features in scenario_features.values():
        combined.extend(features)
    return np.array(combined)

def analyze_person_disability(all_person_features):
    # Extrage caracteristicile pentru fiecare persoanƒÉ
    feature_vectors = []
    person_ids = []
    
    for person_id, features in all_person_features.items():
        feature_vectors.append(features)
        person_ids.append(person_id)
    
    feature_matrix = np.array(feature_vectors)
    
    # NormalizeazƒÉ caracteristicile
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(feature_matrix)
    
    # METODA 1: PCA + DBSCAN pentru detectarea de anomalii
    pca = PCA(n_components=0.95)
    principal_components = pca.fit_transform(scaled_features)
    
    # Clusterizare DBSCAN pentru detectia de anomalii
    clustering = DBSCAN(eps=1.5, min_samples=2).fit(principal_components)
    labels = clustering.labels_
    
    # CalculeazƒÉ distan»õe fa»õƒÉ de centroid
    centroid = np.mean(principal_components, axis=0)
    distances = cdist(principal_components, [centroid]).flatten()
    
    # METODA 2: Analiza multivariatƒÉ pentru detectarea outlier-elor
    # CalculeazƒÉ scorul Mahalanobis pentru fiecare persoanƒÉ
    try:
        # Covarian»õa matricei de caracteristici
        cov_matrix = np.cov(scaled_features.T)
        
        # VerificƒÉ dacƒÉ matricea de covarian»õƒÉ este inversabilƒÉ »ôi are valori finite
        if np.linalg.det(cov_matrix) > 1e-10 and np.all(np.isfinite(cov_matrix)):
            inv_cov_matrix = np.linalg.inv(cov_matrix)
            
            # Scorul Mahalanobis pentru fiecare persoanƒÉ
            mahalanobis_scores = []
            for i, features in enumerate(scaled_features):
                diff = features - np.mean(scaled_features, axis=0)
                mahal_score = np.sqrt(diff.T @ inv_cov_matrix @ diff)
                
                # VerificƒÉ dacƒÉ scorul este valid
                if np.isnan(mahal_score) or np.isinf(mahal_score):
                    mahal_score = distances[i]  # Fallback la distan»õa euclidianƒÉ
                
                mahalanobis_scores.append(mahal_score)
            
            mahalanobis_scores = np.array(mahalanobis_scores)
        else:
            # DacƒÉ matricea de covarian»õƒÉ nu este inversabilƒÉ, folose»ôte distan»õele euclidiene
            mahalanobis_scores = distances
    except:
        # DacƒÉ matricea de covarian»õƒÉ nu este inversabilƒÉ, folose»ôte distan»õele euclidiene
        mahalanobis_scores = distances
    
    # METODA 3: Analiza de consisten»õƒÉ √Æntre scenarii
    # CalculeazƒÉ c√¢t de consistent este comportamentul fiecƒÉrei persoane
    consistency_scores = []
    for i, features in enumerate(scaled_features):
        # CalculeazƒÉ c√¢t de departe este de alte persoane
        other_features = np.vstack([scaled_features[j] for j in range(len(scaled_features)) if j != i])
        if len(other_features) > 0:
            # Distan»õa medie fa»õƒÉ de alte persoane
            avg_distance_to_others = np.mean(cdist([features], other_features))
            consistency_scores.append(avg_distance_to_others)
        else:
            consistency_scores.append(0)
    
    consistency_scores = np.array(consistency_scores)
    
    # COMBINƒÇ TOATE METODELE PENTRU UN SCOR FINAL
    # NormalizeazƒÉ toate scorurile la [0,1] cu verificƒÉri pentru a evita division by zero
    max_distance = np.max(distances) if len(distances) > 0 and np.max(distances) > 0 else 1
    max_mahal = np.max(mahalanobis_scores) if len(mahalanobis_scores) > 0 and np.max(mahalanobis_scores) > 0 else 1
    max_consistency = np.max(consistency_scores) if len(consistency_scores) > 0 and np.max(consistency_scores) > 0 else 1
    
    # Scorul final combinat (media ponderatƒÉ)
    final_scores = []
    normalized_scores = []  # StocheazƒÉ scorurile normalizate pentru fiecare persoanƒÉ
    
    for i in range(len(person_ids)):
        # NormalizeazƒÉ scorurile cu verificƒÉri pentru a evita NaN
        norm_distance = distances[i] / max_distance if max_distance > 0 else 0
        norm_mahal = mahalanobis_scores[i] / max_mahal if max_mahal > 0 else 0
        norm_consistency = consistency_scores[i] / max_consistency if max_consistency > 0 else 0
        
        # VerificƒÉ dacƒÉ scorurile sunt valide (nu NaN sau inf)
        if np.isnan(norm_distance) or np.isinf(norm_distance):
            norm_distance = 0
        if np.isnan(norm_mahal) or np.isinf(norm_mahal):
            norm_mahal = 0
        if np.isnan(norm_consistency) or np.isinf(norm_consistency):
            norm_consistency = 0
        
        # Media ponderatƒÉ (distan»õa are ponderea cea mai mare)
        final_score = 0.5 * norm_distance + 0.3 * norm_mahal + 0.2 * norm_consistency
        
        # VerificƒÉ dacƒÉ scorul final este valid
        if np.isnan(final_score) or np.isinf(final_score):
            final_score = 0.0
        
        final_scores.append(final_score)
        
        # SalveazƒÉ scorurile normalizate pentru a le folosi mai t√¢rziu
        normalized_scores.append({
            'norm_distance': norm_distance,
            'norm_mahal': norm_mahal,
            'norm_consistency': norm_consistency
        })
    
    final_scores = np.array(final_scores)
    
    # VerificƒÉ »ôi corecteazƒÉ orice scor final care ar putea fi NaN
    for i in range(len(final_scores)):
        if np.isnan(final_scores[i]) or np.isinf(final_scores[i]):
            final_scores[i] = 0.0
    
    # CalculeazƒÉ scor de dizabilitate
    disability_assessment = {}
    
    for i, person_id in enumerate(person_ids):
        # Scorul final normalizat
        disability_score = final_scores[i]
        
        # Status bazat pe scorul final
        if disability_score > 0.7:
            status = 'HIGH'
            disability_level = DISABILITY_MESSAGES['HIGH']
        elif disability_score > 0.5:
            status = 'MEDIUM'
            disability_level = DISABILITY_MESSAGES['MEDIUM']
        elif disability_score > 0.3:
            status = 'LOW'
            disability_level = DISABILITY_MESSAGES['LOW']
        else:
            status = 'NONE'
            disability_level = DISABILITY_MESSAGES['NONE']
        
        # VerificƒÉ »ôi corecteazƒÉ scorurile finale pentru a evita NaN
        final_score_clean = disability_score
        if np.isnan(final_score_clean) or np.isinf(final_score_clean):
            final_score_clean = 0.0
        
        # VerificƒÉ »ôi corecteazƒÉ scorurile individuale
        distance_score_clean = distances[i]
        if np.isnan(distance_score_clean) or np.isinf(distance_score_clean):
            distance_score_clean = 0.0
            
        mahal_score_clean = mahalanobis_scores[i]
        if np.isnan(mahal_score_clean) or np.isinf(mahal_score_clean):
            mahal_score_clean = 0.0
            
        consistency_score_clean = consistency_scores[i]
        if np.isnan(consistency_score_clean) or np.isinf(consistency_score_clean):
            consistency_score_clean = 0.0
        
        # Detalii suplimentare pentru analizƒÉ
        disability_assessment[person_id] = {
            'cluster': int(labels[i]),
            'distance_score': float(distance_score_clean),
            'mahalanobis_score': float(mahal_score_clean),
            'consistency_score': float(consistency_score_clean),
            'final_score': float(final_score_clean),
            'status': status,
            'disability_level': disability_level,
            'disability': status in ['HIGH', 'MEDIUM'],
            'risk_factors': []
        }
        
        # IdentificƒÉ factorii de risc folosind scorurile normalizate salvate
        norm_scores = normalized_scores[i]
        if norm_scores['norm_distance'] > 0.7:
            disability_assessment[person_id]['risk_factors'].append('COMPORTAMENT ABNORMAL')
        if norm_scores['norm_mahal'] > 0.7:
            disability_assessment[person_id]['risk_factors'].append('PATTERN-URI NESTANDARD')
        if norm_scores['norm_consistency'] > 0.7:
            disability_assessment[person_id]['risk_factors'].append('INCONSISTEN»öƒÇ √éNTRE SCENARII')
        
        # Nu mai adƒÉugƒÉm recomandƒÉri
    
    return disability_assessment

def create_disability_report(disability_assessment):
    """CreeazƒÉ un raport detaliat al dizabilitƒÉ»õilor"""
    print("\n" + "="*100)
    print("üîç DETALIAT DISABILITY ASSESSMENT REPORT - ANALIZƒÇ COMPORTAMENTALƒÇ")
    print("="*100)
    
    # GrupeazƒÉ persoanele dupƒÉ status
    high_disability = []
    medium_disability = []
    low_disability = []
    no_disability = []
    
    for person, result in disability_assessment.items():
        if result['status'] == 'HIGH':
            high_disability.append((person, result))
        elif result['status'] == 'MEDIUM':
            medium_disability.append((person, result))
        elif result['status'] == 'LOW':
            low_disability.append((person, result))
        else:
            no_disability.append((person, result))
    
    # Afi»ôeazƒÉ persoanele cu dizabilitƒÉ»õi severe
    if high_disability:
        print(f"\nüî¥ PERSOANE CU DIZABILITƒÇ»öI SEVERE ({len(high_disability)}):")
        print("-" * 70)
        for person, result in sorted(high_disability, key=lambda x: x[1]['final_score'], reverse=True):
            print(f"  ‚Ä¢ {person}:")
            print(f"    Score final: {result['final_score']:.3f} | Cluster: {result['cluster']}")
            print(f"    Nivel: {result['disability_level']}")
            print(f"    Distan»õa: {result['distance_score']:.3f} | Mahalanobis: {result['mahalanobis_score']:.3f}")
            print(f"    Consisten»õa: {result['consistency_score']:.3f}")
            if result['risk_factors']:
                print(f"    Factorii de risc: {', '.join(result['risk_factors'])}")
            print()
    
    # Afi»ôeazƒÉ persoanele cu dizabilitƒÉ»õi moderate
    if medium_disability:
        print(f"\nüü† PERSOANE CU DIZABILITƒÇ»öI MODERATE ({len(medium_disability)}):")
        print("-" * 70)
        for person, result in sorted(medium_disability, key=lambda x: x[1]['final_score'], reverse=True):
            print(f"  ‚Ä¢ {person}:")
            print(f"    Score final: {result['final_score']:.3f} | Cluster: {result['cluster']}")
            print(f"    Nivel: {result['disability_level']}")
            print(f"    Distan»õa: {result['distance_score']:.3f} | Mahalanobis: {result['mahalanobis_score']:.3f}")
            print(f"    Consisten»õa: {result['consistency_score']:.3f}")
            if result['risk_factors']:
                print(f"    Factorii de risc: {', '.join(result['risk_factors'])}")
            print()
    
    # Afi»ôeazƒÉ persoanele cu dizabilitƒÉ»õi u»ôoare
    if low_disability:
        print(f"\nüü° PERSOANE CU DIZABILITƒÇ»öI U»òOARE ({len(low_disability)}):")
        print("-" * 70)
        for person, result in sorted(low_disability, key=lambda x: x[1]['final_score'], reverse=True):
            print(f"  ‚Ä¢ {person}:")
            print(f"    Score final: {result['final_score']:.3f} | Cluster: {result['cluster']}")
            print(f"    Nivel: {result['disability_level']}")
            print(f"    Distan»õa: {result['distance_score']:.3f} | Mahalanobis: {result['mahalanobis_score']:.3f}")
            print(f"    Consisten»õa: {result['consistency_score']:.3f}")
            if result['risk_factors']:
                print(f"    Factorii de risc: {', '.join(result['risk_factors'])}")
            print()
    
    # Afi»ôeazƒÉ persoanele fƒÉrƒÉ dizabilitƒÉ»õi
    if no_disability:
        print(f"\nüü¢ PERSOANE FƒÇRƒÇ DIZABILITƒÇ»öI ({len(no_disability)}):")
        print("-" * 70)
        for person, result in sorted(no_disability, key=lambda x: x[1]['final_score']):
            print(f"  ‚Ä¢ {person}:")
            print(f"    Score final: {result['final_score']:.3f} | Cluster: {result['cluster']}")
            print(f"    Nivel: {result['disability_level']}")
            print(f"    Distan»õa: {result['distance_score']:.3f} | Mahalanobis: {result['mahalanobis_score']:.3f}")
            print(f"    Consisten»õa: {result['consistency_score']:.3f}")
            print()
    
    # Statistici generale
    total_persons = len(disability_assessment)
    persons_with_disability = len(high_disability) + len(medium_disability) + len(low_disability)
    percentage_with_disability = (persons_with_disability / total_persons) * 100
    
    print(f"\nüìä STATISTICI GENERALE:")
    print("-" * 50)
    print(f"  ‚Ä¢ Total persoane analizate: {total_persons}")
    print(f"  ‚Ä¢ Persoane cu dizabilitƒÉ»õi severe: {len(high_disability)} ({len(high_disability)/total_persons*100:.1f}%)")
    print(f"  ‚Ä¢ Persoane cu dizabilitƒÉ»õi moderate: {len(medium_disability)} ({len(medium_disability)/total_persons*100:.1f}%)")
    print(f"  ‚Ä¢ Persoane cu dizabilitƒÉ»õi u»ôoare: {len(low_disability)} ({len(low_disability)/total_persons*100:.1f}%)")
    print(f"  ‚Ä¢ Persoane fƒÉrƒÉ dizabilitƒÉ»õi: {len(no_disability)} ({len(no_disability)/total_persons*100:.1f}%)")
    print(f"  ‚Ä¢ Total persoane cu dizabilitƒÉ»õi: {persons_with_disability} ({percentage_with_disability:.1f}%)")
    
    # Analiza cluster-elor
    clusters = {}
    for person, result in disability_assessment.items():
        cluster = result['cluster']
        if cluster not in clusters:
            clusters[cluster] = []
        clusters[cluster].append(person)
    
    print(f"\nüîó ANALIZA CLUSTER-ELOR:")
    print("-" * 30)
    for cluster_id, persons in clusters.items():
        cluster_statuses = [disability_assessment[p]['status'] for p in persons]
        high_count = cluster_statuses.count('HIGH')
        medium_count = cluster_statuses.count('MEDIUM')
        low_count = cluster_statuses.count('LOW')
        none_count = cluster_statuses.count('NONE')
        
        print(f"  ‚Ä¢ Cluster {cluster_id}: {len(persons)} persoane")
        print(f"    - Severe: {high_count}, Moderate: {medium_count}, U»ôoare: {low_count}, FƒÉrƒÉ: {none_count}")
    
    # RecomandƒÉri generale
    print(f"\nüí° RECOMANDƒÇRI GENERALE:")
    print("-" * 30)
    if high_disability:
        print(f"  ‚Ä¢ URGENT: {len(high_disability)} persoane necesitƒÉ evaluare medicalƒÉ imediatƒÉ")
    if medium_disability:
        print(f"  ‚Ä¢ IMPORTANT: {len(medium_disability)} persoane necesitƒÉ monitorizare »ôi evaluare")
    if low_disability:
        print(f"  ‚Ä¢ ATEN»öIE: {len(low_disability)} persoane necesitƒÉ monitorizare u»ôoarƒÉ")
    if no_disability:
        print(f"  ‚Ä¢ NORMAL: {len(no_disability)} persoane au comportament tipic")
    
    # Explica»õie metodologie
    print(f"\nüî¨ METODOLOGIA ANALIZEI:")
    print("-" * 30)
    print(f"  ‚Ä¢ AnalizƒÉ PCA + DBSCAN pentru detectarea de anomalii")
    print(f"  ‚Ä¢ Scorul Mahalanobis pentru pattern-uri nestandard")
    print(f"  ‚Ä¢ Analiza de consisten»õƒÉ √Æntre scenarii A, B, C")
    print(f"  ‚Ä¢ Scorul final combinat (50% distan»õƒÉ + 30% Mahalanobis + 20% consisten»õƒÉ)")
    print(f"  ‚Ä¢ Threshold-uri: >0.7 (sever), >0.5 (moderat), >0.3 (u»ôor), ‚â§0.3 (fƒÉrƒÉ)")
    
    print("="*100)

def cleanup_disability_files():
    """»òterge doar fi»ôierele legate de analiza dizabilitƒÉ»õilor »ôi compara»õiile de scenarii"""
    import os
    import glob
    
    # Lista fi»ôierelor legate de dizabilitƒÉ»õi care trebuie »ôterse
    disability_files = [
        'disability_visual_report.png',
        'detailed_disability_assessment.json',
        'disability_summary.json'
    ]
    
    # Lista fi»ôierelor de compara»õie scenarii care trebuie regenerate
    scenario_comparison_files = [
        'Person_*_scenarios_comparison.png'
    ]
    
    print("üßπ CurƒÉ»õare fi»ôiere legate de dizabilitƒÉ»õi »ôi compara»õii scenarii...")
    
    # »òterge fi»ôierele de dizabilitƒÉ»õi
    for file_pattern in disability_files:
        # CautƒÉ fi»ôierele care se potrivesc pattern-ului
        matching_files = glob.glob(file_pattern)
        for file_path in matching_files:
            try:
                if os.path.exists(file_path):
                    os.remove(file_path)
                    print(f"   ‚úÖ »òters: {file_path}")
            except Exception as e:
                print(f"   ‚ùå Eroare la »ôtergerea {file_path}: {e}")
    
    # »òterge fi»ôierele de compara»õie scenarii
    for file_pattern in scenario_comparison_files:
        # CautƒÉ fi»ôierele care se potrivesc pattern-ului
        matching_files = glob.glob(file_pattern)
        for file_path in matching_files:
            try:
                if os.path.exists(file_path):
                    os.remove(file_path)
                    print(f"   ‚úÖ »òters: {file_path}")
            except Exception as e:
                print(f"   ‚ùå Eroare la »ôtergerea {file_path}: {e}")
    
    print("   üéØ Fi»ôierele din prima variantƒÉ au fost pƒÉstrate!")
    print("   üîÑ Fi»ôierele de compara»õie scenarii vor fi regenerate!")

def main():
    # CurƒÉ»õƒÉ doar fi»ôierele legate de dizabilitƒÉ»õi »ôi compara»õii scenarii
    cleanup_disability_files()
    
    json_files = list(Path('vr_recordings').glob('*.json'))
    if not json_files:
        print("No JSON files found in vr_recordings directory")
        return

    # GrupeazƒÉ fi»ôierele dupƒÉ persoanƒÉ
    person_files = group_files_by_person(json_files)
    
    if not person_files:
        print("No valid person files found")
        return
    
    print(f"Found {len(person_files)} persons with scenarios")
    
    # Debug: afi»ôeazƒÉ ce fi»ôiere au fost gƒÉsite pentru fiecare persoanƒÉ
    for person_id, files in person_files.items():
        print(f"  ‚Ä¢ {person_id}: {[f.name for f in files]}")
    
    all_person_features = {}
    disability_assessment = {}
    
    # ProceseazƒÉ fiecare persoanƒÉ
    for person_id, files in person_files.items():
        print(f"\n{'='*40}")
        print(f"PROCESSING PERSON: {person_id}")
        print(f"{'='*40}")
        
        person_features = {}
        all_data = {}
        global_origin = None
        
        # ProceseazƒÉ fiecare scenariu al persoanei
        for file_path in files:
            scenario_name = file_path.stem
            print(f"\nProcessing scenario: {scenario_name}")
            
            try:
                # √éncarcƒÉ datele
                positions, rotations, forward_vectors, timestamps = load_complete_head_data(str(file_path))

                if len(positions) == 0:
                    print(f"No valid data for scenario {scenario_name}")
                    continue

                # SeteazƒÉ originea globalƒÉ dacƒÉ nu existƒÉ
                if global_origin is None:
                    global_origin = positions[0]

                # NormalizeazƒÉ pozi»õiile
                positions = positions - global_origin

                # Sub-e»ôantioneazƒÉ
                N = 10
                positions = positions[::N]
                rotations = rotations[::N]
                forward_vectors = forward_vectors[::N]
                timestamps = timestamps[::N]

                # SalveazƒÉ datele pentru analiza globalƒÉ
                scenario_data = np.hstack((
                    positions,
                    rotations,
                    forward_vectors,
                    timestamps.reshape(-1, 1)
                ))
                all_data[scenario_name] = scenario_data

                # Extrage caracteristicile pentru scenariu
                features = extract_behavior_features(scenario_data)
                print(f"      ‚úÖ Caracteristici extrase: {len(features)} features")
                person_features[scenario_name] = features
                
                # VerificƒÉ dacƒÉ fi»ôierele de analizƒÉ individualƒÉ existƒÉ deja
                individual_analysis_file = f"detailed_analysis_{scenario_name}_3d.png"
                if not os.path.exists(individual_analysis_file):
                    print(f"   üîÑ Generare analizƒÉ individualƒÉ pentru {scenario_name}...")
                    create_detailed_head_analysis(file_path, save_plot=True, global_origin=global_origin, show_plot=False)
                else:
                    print(f"   ‚úÖ Analiza individualƒÉ pentru {scenario_name} existƒÉ deja")

            except Exception as e:
                print(f"Error processing {scenario_name}: {e}")
        
        # CombinƒÉ caracteristicile din cele 3 scenarii
        if person_features:
            print(f"   üîÑ Combinare caracteristici pentru {person_id}...")
            combined_features = combine_features_for_person(person_features)
            print(f"      ‚úÖ Caracteristici combinate: {len(combined_features)} features")
            all_person_features[person_id] = combined_features
            
            # Plot comparativ pentru scenariile aceleia»ôi persoane
            if all_data:
                scenario_comparison_file = f"{person_id}_scenarios_comparison.png"
                if not os.path.exists(scenario_comparison_file):
                    print(f"   üîÑ Generare compara»õie scenarii pentru {person_id}...")
                    plot_all_scenarios_comparison(all_data, global_origin=global_origin)
                    plt.savefig(scenario_comparison_file, dpi=300)
                    plt.close()
                else:
                    print(f"   ‚úÖ Compara»õia scenarii pentru {person_id} existƒÉ deja")
    
    # AnalizeazƒÉ dizabilitƒÉ»õile pe baza caracteristicilor combinate
    if all_person_features:
        print(f"\nüîç ANALIZA DIZABILITƒÇ»öILOR...")
        print(f"   ‚Ä¢ Caracteristici disponibile pentru {len(all_person_features)} persoane")
        
        # Debug: afi»ôeazƒÉ ce persoane au caracteristici
        for person_id, features in all_person_features.items():
            print(f"      - {person_id}: {len(features)} caracteristici")
        
        disability_assessment = analyze_person_disability(all_person_features)
        print(f"   ‚Ä¢ Analiza dizabilitƒÉ»õilor finalizatƒÉ pentru {len(disability_assessment)} persoane")
        
        # SalveazƒÉ rezultatele
        print(f"   üíæ Salvare rezultate...")
        save_detailed_results(disability_assessment, all_person_features)
        
        print(f"   üìä Generare raport vizual...")
        create_visual_disability_report(disability_assessment)
        
        # Afi»ôeazƒÉ rezultatele finale
        print(f"   üìã Afi»ôare raport final...")
        create_disability_report(disability_assessment)
        
        print(f"\nüéâ ANALIZA COMPLETƒÇ FINALIZATƒÇ!")
        print(f"   ‚Ä¢ {len(person_files)} persoane procesate")
        print(f"   ‚Ä¢ Fi»ôierele din prima variantƒÉ au fost pƒÉstrate")
        print(f"   ‚Ä¢ Fi»ôierele de compara»õie scenarii au fost regenerate")
        print(f"   ‚Ä¢ Imaginile de dizabilitƒÉ»õi au fost regenerate cu graficele corecte")
    else:
        print(f"\n‚ùå NU SUNT CARACTERISTICI DISPONIBILE PENTRU ANALIZA DIZABILITƒÇ»öILOR!")
        print(f"   ‚Ä¢ VerificƒÉ dacƒÉ datele au fost procesate corect")
        print(f"   ‚Ä¢ VerificƒÉ dacƒÉ fi»ôierele JSON con»õin date valide")

if __name__ == "__main__":
    main()